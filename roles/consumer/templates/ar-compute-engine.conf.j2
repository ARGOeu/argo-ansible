[default]

# mongo server ip location
mongo_host={{ mongo_host_or_ip }}

# mongo server port
mongo_port={{ mongo_port_number }}

# core database used by argo
mongo_core_db = argo_core

# mongo authentication
# mongo_user =
# mongo_pass =

# declare the mode of ARGOeu
# can be: local or cluster
mode={{ argo_compute_mode }}

# declare the serialization framework
# can be: avro or none
serialization=none

# declare if prefilter data must be cleaned after upload to hdfs
prefilter_clean={{ prefilter_clean_bool }}
sync_clean=true

# Provide maximum number of recomputations that can run in parallel.
recomp_threshold=1

[logging]

# mode for logging (syslog,file,none)
log_mode=syslog

# log level status
log_level=DEBUG

# If log_mode equals file - uncomment to set log file path:
# log_file=/var/log/ar-compute/ar-compute.log

# Hadoop clients log level and log appender
# If you want to log via SYSLOG make sure
# an appropriate appender is defined in hadoop
# log4j.properties file and just add the name
# of this appender in the following line. I.e.
# if you define a new appender named SYSLOG
# change console to SYSLOG, or just add
# SYSLOG appender in the following line
hadoop_log_root=INFO,console

[connectors]

sync_exec={{ argo_exec_path }}
sync_path={{ argo_sync_path }}

[jobs]

# Here are declared available tenants and available jobs
# for each tenant (tenant/job names are case-sensitive)
# The order of declarations is as follows:
#
# tenants=TenantA,TenantB
# TenantA_jobs=Job1,Job2,Job3
# TenantB_jobs=Job4,Job5
# TenantA_prefilter=prefilter_exec (optional)
#
# Declare available tenants
tenants={{ tenants|join(',')}}

# For a declared tenant declare it's jobs by using
# {Tenant_Name}_jobs conformance
{% for key,value in tenants.iteritems() %}
{{ key }}_jobs={{ value.jobs_all|replace(" ","") }}
{% if value.prefilter is defined %}
{{ key }}_prefilter={{ value.prefilter }}
{% endif %}
{% endfor %}


[sampling]

s_period=1440
s_interval=5